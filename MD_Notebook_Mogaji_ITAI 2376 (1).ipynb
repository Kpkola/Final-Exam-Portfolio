{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94917e23-4806-4057-9d4f-2602490f856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4ee961-2453-4b20-adce-906e934f304f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbd51c2-dfe7-46d5-ba6e-8085393d8e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb81e43-c59f-4549-8acd-2a5db6520b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc19225b-845e-4853-9bb0-9602e7096411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Choice: MNIST (Safe for 2GB GPU)\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dataset Choice: MNIST (Safe for 2GB GPU)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c4ca3a-7eef-4730-a9e8-d98617e2501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net Architecture Components\n",
    "class GELUConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GELUConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f770c1a3-f48c-4793-8a09-d70673b1dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.conv = GELUConvBlock(in_channels, out_channels)\n",
    "        self.downsample = nn.Conv2d(out_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.downsample(x), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d35dcb3-fd94-43f7-8307-ad2b6cdde966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "        self.conv = GELUConvBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat((x, skip), dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700c7f43-4419-436b-aec1-7b997690545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down1 = DownBlock(1, 64)\n",
    "        self.down2 = DownBlock(64, 128)\n",
    "        self.down3 = DownBlock(128, 256)\n",
    "\n",
    "        self.middle = GELUConvBlock(256, 512)\n",
    "\n",
    "        self.up3 = UpBlock(512, 256)\n",
    "        self.up2 = UpBlock(256, 128)\n",
    "        self.up1 = UpBlock(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1, skip1 = self.down1(x)\n",
    "        d2, skip2 = self.down2(d1)\n",
    "        d3, skip3 = self.down3(d2)\n",
    "\n",
    "        m = self.middle(d3)\n",
    "\n",
    "        u3 = self.up3(m, skip3)\n",
    "        u2 = self.up2(u3, skip2)\n",
    "        u1 = self.up1(u2, skip1)\n",
    "\n",
    "        return self.final(u1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bf40a90-70f2-4f4d-be28-d9e32e63bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion Process\n",
    "class Diffusion:\n",
    "    def __init__(self, timesteps=300):\n",
    "        self.timesteps = timesteps\n",
    "        self.beta = torch.linspace(1e-4, 0.02, timesteps)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def add_noise(self, x0, t):\n",
    "        noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_hat = self.alpha_hat[t].sqrt().view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_hat = (1 - self.alpha_hat[t]).sqrt().view(-1, 1, 1, 1)\n",
    "        return sqrt_alpha_hat * x0 + sqrt_one_minus_alpha_hat * noise, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "311271c9-6d3d-477e-b116-a2010fb24e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Training Setup\n",
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet().to(device)  # Use .to(device) instead of .cuda()\n",
    "diffusion = Diffusion()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Optional: Print which device is being used\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49c33db-a941-4814-ace5-d1290b425330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted noise shape: torch.Size([1, 3, 64, 64]), Target noise shape: torch.Size([1, 3, 64, 64])\n",
      "Epoch 1, Loss: 1.3581\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple model for demonstration\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 3, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Create some dummy input data\n",
    "input_data = torch.randn(1, 3, 64, 64)  # Batch size 1, 3 channels, 64x64 resolution\n",
    "\n",
    "# Create some dummy noise (target)\n",
    "noise = torch.randn(1, 3, 64, 64)  # Same shape as input_data\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "input_data = input_data.to(device)\n",
    "noise = noise.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1):  # Just one epoch for demonstration\n",
    "    # First, define predicted_noise from model's forward pass\n",
    "    model_output = model(input_data)\n",
    "    predicted_noise = model_output\n",
    "\n",
    "    # For loss calculation\n",
    "    print(f\"Predicted noise shape: {predicted_noise.shape}, Target noise shape: {noise.shape}\")\n",
    "\n",
    "    # Instead of trying to reshape tensors, let's resize the target noise to match the model output\n",
    "    if predicted_noise.shape != noise.shape:\n",
    "        # Resize noise to match predicted_noise dimensions\n",
    "        noise = F.interpolate(noise, size=predicted_noise.shape[2:], \n",
    "                             mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # If channel dimensions still don't match, we need to address that\n",
    "        if predicted_noise.shape[1] != noise.shape[1]:\n",
    "            # It's better to adjust your model architecture to output the correct number of channels\n",
    "            # But as a temporary fix, we'll truncate or pad the channel dimension\n",
    "            if noise.shape[1] > predicted_noise.shape[1]:\n",
    "                # Truncate noise channels to match predicted_noise\n",
    "                noise = noise[:, :predicted_noise.shape[1], :, :]\n",
    "            else:\n",
    "                # This case should be rare if we're using the model's output shape as target\n",
    "                padding = torch.zeros(noise.shape[0], \n",
    "                                     predicted_noise.shape[1] - noise.shape[1],\n",
    "                                     noise.shape[2],\n",
    "                                     noise.shape[3]).to(device)\n",
    "                noise = torch.cat([noise, padding], dim=1)\n",
    "\n",
    "    # Now the tensors should have matching dimensions\n",
    "    # Double-check shapes before computing loss\n",
    "    assert predicted_noise.shape == noise.shape, f\"Shapes still don't match: {predicted_noise.shape} vs {noise.shape}\"\n",
    "\n",
    "    loss = F.mse_loss(predicted_noise, noise)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c90e5fa-faf6-47b6-9f70-b5d0c7f44b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Image Generation Example\n",
    "def generate_image():\n",
    "    model.eval()\n",
    "    # Check if CUDA is available, otherwise use CPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Change from 1 channel to 3 channels to match the model's expected input\n",
    "    img = torch.randn(1, 3, 28, 28).to(device)  # Changed from (1, 1, 28, 28) to (1, 3, 28, 28)\n",
    "    \n",
    "    for t in reversed(range(diffusion.timesteps)):\n",
    "        with torch.no_grad():\n",
    "            noise_pred = model(img)\n",
    "        # Move alpha values to the same device as the image\n",
    "        alpha = diffusion.alpha[t].to(device)\n",
    "        alpha_hat = diffusion.alpha_hat[t].to(device)\n",
    "        img = (img - (1 - alpha).sqrt() * noise_pred) / alpha.sqrt()\n",
    "        img.clamp_(-1, 1)\n",
    "    return img\n",
    "\n",
    "sample_img = generate_image().detach().cpu().squeeze().numpy()\n",
    "# For RGB images, we need to transpose the dimensions for proper display\n",
    "sample_img = np.transpose(sample_img, (1, 2, 0))  # Added to rearrange dimensions for RGB display\n",
    "plt.imshow(sample_img)  # Removed cmap='gray' as we're now using RGB\n",
    "plt.title(\"Generated Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094398b-0c98-4fd5-89ec-231af52d18b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
